{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dacon  5 회 생체 광학 데이터 분석 AI 모델링 경진대회\n",
    "## 초보\n",
    "## 2020년 7월 03일"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 라이브러리 및 데이터\n",
    "## Library & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# LOAD LIBRARIES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle, os\n",
    "import joblib\n",
    "from tqdm import trange, tqdm, tqdm_notebook\n",
    "\n",
    "# DATA SPLIT\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "# EVALUATE\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# MODEL\n",
    "import lightgbm as lgb\n",
    "\n",
    "# ELSE\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "os.getcwd()\n",
    "os.chdir('C:\\\\Users\\\\ParkGiChan\\\\Desktop\\\\DataAnalysis\\\\DACON_BIO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 변수 선택 및 모델 구축\n",
    "## Feature Engineering & Initial Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train.csv')\n",
    "test = pd.read_csv('./test.csv')\n",
    "submission = pd.read_csv('./sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_col=train.columns[train.columns.str.contains('src')]\n",
    "dst_col = train.columns[train.columns.str.contains('dst')]\n",
    "\n",
    "def simple_fe(train):\n",
    "    \"\"\"\n",
    "    전체 파장에 대한 src, dst 평균 / mean 투과도 / std 투과도\n",
    "    \"\"\"\n",
    "    train['src_mean']=train[src_col].mean(1)\n",
    "    train['dst_mean']=train[dst_col].mean(1)\n",
    "    train['Trans_mean'] = train['dst_mean']/train['src_mean']\n",
    "\n",
    "    train['src_std']=train[src_col].std(1)\n",
    "    train['dst_std']=train[dst_col].std(1)\n",
    "    train['Trans_std'] = train['dst_std']/train['src_std']\n",
    "\n",
    "    \n",
    "    return train\n",
    "\n",
    "train = simple_fe(train)\n",
    "test = simple_fe(test)\n",
    "\n",
    "############################################################\n",
    "\n",
    "def binning_fe(train, src_col, dst_col, size=50):\n",
    "    \"\"\"\n",
    "    size 별로 binning후 피쳐 생성\n",
    "    \"\"\"\n",
    "    for i in range(650, 1000-size, size):\n",
    "        temp1=train.loc[:, '%s_src'%i : '%s_src'%(i+size)].mean(1)\n",
    "        temp2=train.loc[:, '%s_dst'%i : '%s_dst'%(i+size)].mean(1)\n",
    "        train['Trans_%s_to_%s_mean'%(i,i+size)] = temp2/temp1\n",
    "        train['Concen_%s_to_%s_mean'%(i,i+size)] = np.log(temp2/temp1) / train['rho']\n",
    "    \n",
    "    return train\n",
    "\n",
    "for i in [10, 20,30,40, 50,100]:\n",
    "    train = binning_fe(train, src_col, dst_col, size=i)\n",
    "    test = binning_fe(test, src_col, dst_col, size=i)\n",
    "############################################################\n",
    "\n",
    "def rolling_fe(train):\n",
    "    \"\"\"\n",
    "    window size별 롤링후 피쳐 생성\n",
    "    \"\"\"\n",
    "    temp = [x for x in range(650,1000,10)]\n",
    "    for k in [3, 5]:# window size\n",
    "        temp1 = train[src_col].rolling(window = k, min_periods=1, axis=1).mean()\n",
    "        temp2 = train[dst_col].rolling(window = k, min_periods=1, axis=1).mean()\n",
    "        for i in temp:\n",
    "            train['%s_rolling_size_%s'%(i, k)] = np.log(temp2['%s_dst'%i]/temp1['%s_src'%i])/train['rho']\n",
    "        \n",
    "    return train\n",
    "\n",
    "train = rolling_fe(train)\n",
    "test = rolling_fe(test)\n",
    "\n",
    "############################################################\n",
    "\n",
    "def rolling_fe2(train):\n",
    "    \"\"\"\n",
    "    3, 5 각 윈도우 사이즈로 만든 피쳐끼리 나누기\n",
    "    \"\"\"\n",
    "    temp = [x for x in range(650,1000,10)]\n",
    "    for i in temp:\n",
    "        train['%s_rolling_size_3/5'%i] = train['%s_rolling_size_3'%i]/ train['%s_rolling_size_5'%i]\n",
    "        \n",
    "    return train\n",
    "\n",
    "train = rolling_fe2(train)\n",
    "test = rolling_fe2(test)\n",
    "\n",
    "############################################################\n",
    "def rolling_fe3(train, near):\n",
    "    \"\"\"\n",
    "    서로 가까운 영역의 롤링 피쳐들끼리 나눠주기\n",
    "    \"\"\"\n",
    "    temp = [x for x in range(650,1000-near, 10)]\n",
    "    for i in temp:\n",
    "        train['near_%s_%s_size_3'%(i,i+near)] = train['%s_rolling_size_%s'%(i, 3)] / train['%s_rolling_size_%s'%(i+near, 3)]\n",
    "        train['near_%s_%s_size_5'%(i,i+near)] = train['%s_rolling_size_%s'%(i, 5)] / train['%s_rolling_size_%s'%(i+near, 5)]\n",
    "    return train\n",
    "\n",
    "for i in range(10, 30, 10):\n",
    "    train = rolling_fe3(train,i)\n",
    "    test = rolling_fe3(test, i)\n",
    "############################################################\n",
    "\n",
    "def core_fe(train):\n",
    "    \"\"\"\n",
    "    파장대별로 투과도 피쳐와, 농도와 직결되는 피쳐 생성\n",
    "    \"\"\"\n",
    "    temp = [x for x in range(650,1000,10)]\n",
    "    for i in temp:\n",
    "        train['Trans_%s'%i] = train['%s_dst'%i]/train['%s_src'%i]\n",
    "        train['Concen_%s'%i] = np.log(train['%s_dst'%i]/train['%s_src'%i])/train['rho']\n",
    "    return train\n",
    "\n",
    "train = core_fe(train)\n",
    "test = core_fe(test)\n",
    "\n",
    "############################################################\n",
    "\n",
    "def core_fe2(train):\n",
    "    \"\"\"\n",
    "    모든 파장대의 피쳐끼리 서로 빼기 나누기\n",
    "    \"\"\"\n",
    "    temp = [x for x in range(650,1000,10)]\n",
    "    temp_col = []\n",
    "    for i in temp:\n",
    "        temp_col.append('Concen_%s'%i)\n",
    "\n",
    "    for i in temp_col:\n",
    "        for j in temp_col:\n",
    "            if i!=j:\n",
    "                train['%s_%s_near_all_div'%(i,j)] = train[i]/train[j]\n",
    "    \n",
    "    return train\n",
    "\n",
    "train = core_fe2(train)\n",
    "test = core_fe2(test)\n",
    "\n",
    "############################################################\n",
    "\n",
    "def core_fe3(train):\n",
    "    \"\"\"\n",
    "    모든 파장대의 피쳐끼리 서로 빼기 나누기\n",
    "    \"\"\"\n",
    "    temp = [x for x in range(650,1000,10)]\n",
    "    temp_col = []\n",
    "    for i in temp:\n",
    "        temp_col.append('Concen_%s'%i)\n",
    "\n",
    "    for i in temp_col:\n",
    "        for j in temp_col:\n",
    "            if i!=j:\n",
    "                train['%s_%s_near_all_sub'%(i,j)] = train[i]-train[j]\n",
    "    \n",
    "    return train\n",
    "\n",
    "train = core_fe3(train)\n",
    "test = core_fe3(test)\n",
    "############################################################\n",
    "def core_fe4(train, size=10):\n",
    "    temp_col=[]\n",
    "    for i in [x for x in range(650, 1000-size, size)]:\n",
    "        temp_col.append('Concen_%s_to_%s_mean'%(i,i+size))\n",
    "        \n",
    "    for i in temp_col:\n",
    "        for j in temp_col:\n",
    "            if i!=j:\n",
    "                train['temp_%s_%s'%(i,j)] = train[i]/train[j]\n",
    "                train['temp2_%s_%s'%(i,j)] = train[i]-train[j]\n",
    "    return train\n",
    "\n",
    "for i in [10,20,30,40,50]:\n",
    "    train = core_fe4(train, size=i)\n",
    "    test = core_fe4(test, size=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_features=['id','hhb', 'hbo2', 'ca', 'na']\n",
    "col = [x for x in train.columns if x not in excluded_features]\n",
    "\n",
    "x_train = train[col]\n",
    "y_train = train.loc[:, 'hhb':'na']\n",
    "test = test[col]\n",
    "\n",
    "x_train=x_train.replace([np.inf, -np.inf], np.nan)\n",
    "test=test.replace([np.inf, -np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 모델 학습 및 검증\n",
    "## Model Tuning & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from eli5.permutation_importance import get_score_importances\n",
    "\n",
    "X_train = x_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_permutation_badfeatures(X_train, y_train, col):\n",
    "    y_train_temp = y_train[col].copy()\n",
    "    threshold = [0.0001]\n",
    "    bad_features1= []\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        \n",
    "    def score(X, y):\n",
    "        y_pred = reg.predict(X)\n",
    "        return abs(y-y_pred).mean() \n",
    "    \n",
    "    for n_fold, (trn_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "        trn_x, trn_y = X_train.iloc[trn_idx], y_train_temp.iloc[trn_idx]\n",
    "        val_x, val_y = X_train.iloc[val_idx], y_train_temp.iloc[val_idx]\n",
    "        \n",
    "        reg= lgb.LGBMRegressor(boosting_type ='gbdt',n_estimators=20000,num_leaves=32, max_depth=-1, min_child_weight=5, \n",
    "                                 subsample=0.7, colsample_bytree =1, learning_rate=0.01, gamma = 0 , n_jobs=-1,\n",
    "                            random_state=42,reg_alpha=0.1, reg_lambda=0.1)\n",
    "\n",
    "        reg.fit(trn_x, trn_y, eval_set=[(trn_x, trn_y),(val_x, val_y)], \n",
    "                  early_stopping_rounds=50 ,verbose=-1, eval_metric='mae')\n",
    "        \n",
    "        \n",
    "        \n",
    "        base_score, score_decreases = get_score_importances(score,np.array(val_x), np.array(val_y), n_iter=1)\n",
    "        \n",
    "        bad_features1.extend(list(val_x.columns[score_decreases[0] > -threshold[0]]))\n",
    "                             \n",
    "    return bad_features1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in y_train.columns:\n",
    "    bad_features = my_permutation_badfeatures(X_train, y_train, col)\n",
    "    pd.DataFrame(bad_features)[0].value_counts().to_csv(\"./bad_features/%s_bad_features.csv\"%col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## permutation 후 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute_train_model(x_train, y_train, x_test, label):\n",
    "    models=[]\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    oof = np.zeros((x_train.shape[0],)) # oof\n",
    "    pred = np.zeros(x_test.shape[0])\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    \n",
    "    # train, test split\n",
    "    for n_fold, (trn_idx, val_idx) in enumerate(kf.split(x_train)):\n",
    "\n",
    "        #print(n_fold)\n",
    "        trn_x, trn_y = x_train.iloc[trn_idx], y_train.iloc[trn_idx]\n",
    "        val_x, val_y = x_train.iloc[val_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        if os.path.isfile('./models/%s_%sfold_0.78230.pkl'%(label, n_fold)): # 모델 존재할경우\n",
    "            print('%s model load..'%label)\n",
    "            model= joblib.load('./models/%s_%sfold_0.78230.pkl'%(label,n_fold))\n",
    "        \n",
    "        else: # 모델 없을때는 train\n",
    "            print('train %s '%label)\n",
    "            model= lgb.LGBMRegressor(boosting_type ='dart',n_estimators=50000,num_leaves=64, max_depth=-1, min_child_weight=5, \n",
    "                                     subsample=0.7, colsample_bytree = 0.2, learning_rate=0.01, gamma = 0 , n_jobs=-1,\n",
    "                                random_state=42,reg_alpha=0.1, reg_lambda=0.1)\n",
    "\n",
    "            model.fit(trn_x, trn_y, eval_set=[(trn_x, trn_y),(val_x, val_y)], \n",
    "                      early_stopping_rounds=50 ,verbose=50000, eval_metric='mae')\n",
    "        \n",
    "            models.append(model)\n",
    "        \n",
    "\n",
    "        # OOF\n",
    "        v_p = model.predict(val_x)\n",
    "        oof[val_idx] = v_p\n",
    "    \n",
    "        # PREDS\n",
    "        pred += model.predict(x_test)/5.0\n",
    "        \n",
    "    return models, oof, pred\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280\n",
      "hhb model load\n",
      "hhb model load\n",
      "hhb model load\n",
      "hhb model load\n",
      "hhb model load\n",
      "236\n",
      "hbo2 model load\n",
      "hbo2 model load\n",
      "hbo2 model load\n",
      "hbo2 model load\n",
      "hbo2 model load\n",
      "278\n",
      "ca model load\n",
      "ca model load\n",
      "ca model load\n",
      "ca model load\n",
      "ca model load\n",
      "162\n",
      "na model load\n",
      "na model load\n",
      "na model load\n",
      "na model load\n",
      "na model load\n"
     ]
    }
   ],
   "source": [
    "# bad_features는 ~~ github주소\n",
    "permute_models = {}\n",
    "permute_oofs=[]\n",
    "permute_preds=[]\n",
    "for label in y_train.columns:\n",
    "    pred = np.zeros(test.shape[0])\n",
    "    \n",
    "    permute_features = pd.read_csv('./bad_features/%s_bad_features.csv'%label,  names=['feature','count'])   \n",
    "    bad_features = permute_features[permute_features['count'] >1]['feature'].values \n",
    "    \n",
    "    x_reduced_trn = x_train.drop(bad_features, axis=1).copy()\n",
    "    x_reduced_test = test.drop(bad_features, axis=1).copy()\n",
    "\n",
    "    ms, oof, pred =permute_train_model(x_reduced_trn, y_train[label], x_reduced_test, label)\n",
    "\n",
    "    permute_models[label] = ms\n",
    "    permute_oofs.append(oof)\n",
    "    permute_preds.append(pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49469949544364755, 0.3951968527521723, 1.2320512842905125, 1.007247908003161]\n",
      "oof mae 0.78230\n"
     ]
    }
   ],
   "source": [
    "labels = ['hhb', 'hbo2', 'ca', 'na']\n",
    "\n",
    "a=[]\n",
    "for label,i in zip(labels,permute_oofs):\n",
    "    a.append(mean_absolute_error(train[label], i))\n",
    "print(a)\n",
    "print('oof mae %.5f'% np.mean(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>hhb</th>\n",
       "      <th>hbo2</th>\n",
       "      <th>ca</th>\n",
       "      <th>na</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000</td>\n",
       "      <td>8.240995</td>\n",
       "      <td>4.874584</td>\n",
       "      <td>9.376731</td>\n",
       "      <td>3.102264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>7.436999</td>\n",
       "      <td>2.485243</td>\n",
       "      <td>8.694831</td>\n",
       "      <td>2.330186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10002</td>\n",
       "      <td>9.766639</td>\n",
       "      <td>5.094046</td>\n",
       "      <td>9.731256</td>\n",
       "      <td>3.201371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10003</td>\n",
       "      <td>8.112722</td>\n",
       "      <td>4.391882</td>\n",
       "      <td>9.359735</td>\n",
       "      <td>4.386559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10004</td>\n",
       "      <td>6.892838</td>\n",
       "      <td>2.795613</td>\n",
       "      <td>9.154041</td>\n",
       "      <td>3.434072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>19995</td>\n",
       "      <td>3.650341</td>\n",
       "      <td>4.473906</td>\n",
       "      <td>12.157714</td>\n",
       "      <td>5.282470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>19996</td>\n",
       "      <td>10.640541</td>\n",
       "      <td>2.935168</td>\n",
       "      <td>8.345288</td>\n",
       "      <td>2.959774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>19997</td>\n",
       "      <td>6.784586</td>\n",
       "      <td>3.711216</td>\n",
       "      <td>9.008868</td>\n",
       "      <td>3.962321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>19998</td>\n",
       "      <td>12.838950</td>\n",
       "      <td>4.575061</td>\n",
       "      <td>8.045664</td>\n",
       "      <td>4.044267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>19999</td>\n",
       "      <td>4.030887</td>\n",
       "      <td>5.662293</td>\n",
       "      <td>9.853273</td>\n",
       "      <td>2.191664</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id        hhb      hbo2         ca        na\n",
       "0     10000   8.240995  4.874584   9.376731  3.102264\n",
       "1     10001   7.436999  2.485243   8.694831  2.330186\n",
       "2     10002   9.766639  5.094046   9.731256  3.201371\n",
       "3     10003   8.112722  4.391882   9.359735  4.386559\n",
       "4     10004   6.892838  2.795613   9.154041  3.434072\n",
       "...     ...        ...       ...        ...       ...\n",
       "9995  19995   3.650341  4.473906  12.157714  5.282470\n",
       "9996  19996  10.640541  2.935168   8.345288  2.959774\n",
       "9997  19997   6.784586  3.711216   9.008868  3.962321\n",
       "9998  19998  12.838950  4.575061   8.045664  4.044267\n",
       "9999  19999   4.030887  5.662293   9.853273  2.191664\n",
       "\n",
       "[10000 rows x 5 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# permute 용도\n",
    "submission = pd.read_csv('./sample_submission.csv')\n",
    "submission['hhb'] = permute_preds[0]\n",
    "submission['hbo2'] = permute_preds[1]\n",
    "submission['ca'] = permute_preds[2]\n",
    "submission['na'] = permute_preds[3]\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>hhb</th>\n",
       "      <th>hbo2</th>\n",
       "      <th>ca</th>\n",
       "      <th>na</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000</td>\n",
       "      <td>8.385180</td>\n",
       "      <td>4.951731</td>\n",
       "      <td>9.617316</td>\n",
       "      <td>2.969566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>7.517709</td>\n",
       "      <td>2.465483</td>\n",
       "      <td>8.854714</td>\n",
       "      <td>2.409327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10002</td>\n",
       "      <td>9.706019</td>\n",
       "      <td>5.112048</td>\n",
       "      <td>9.844248</td>\n",
       "      <td>3.254742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10003</td>\n",
       "      <td>8.162751</td>\n",
       "      <td>4.326323</td>\n",
       "      <td>9.363771</td>\n",
       "      <td>4.632428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10004</td>\n",
       "      <td>6.549421</td>\n",
       "      <td>2.967990</td>\n",
       "      <td>8.919735</td>\n",
       "      <td>3.397089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>19995</td>\n",
       "      <td>3.572231</td>\n",
       "      <td>4.502444</td>\n",
       "      <td>11.946385</td>\n",
       "      <td>5.093107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>19996</td>\n",
       "      <td>10.622650</td>\n",
       "      <td>2.911157</td>\n",
       "      <td>8.060776</td>\n",
       "      <td>3.144995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>19997</td>\n",
       "      <td>6.858170</td>\n",
       "      <td>3.687651</td>\n",
       "      <td>8.885298</td>\n",
       "      <td>3.943446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>19998</td>\n",
       "      <td>12.892524</td>\n",
       "      <td>4.579880</td>\n",
       "      <td>8.060315</td>\n",
       "      <td>3.944395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>19999</td>\n",
       "      <td>4.042493</td>\n",
       "      <td>5.651729</td>\n",
       "      <td>9.960974</td>\n",
       "      <td>2.283841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id        hhb      hbo2         ca        na\n",
       "0     10000   8.385180  4.951731   9.617316  2.969566\n",
       "1     10001   7.517709  2.465483   8.854714  2.409327\n",
       "2     10002   9.706019  5.112048   9.844248  3.254742\n",
       "3     10003   8.162751  4.326323   9.363771  4.632428\n",
       "4     10004   6.549421  2.967990   8.919735  3.397089\n",
       "...     ...        ...       ...        ...       ...\n",
       "9995  19995   3.572231  4.502444  11.946385  5.093107\n",
       "9996  19996  10.622650  2.911157   8.060776  3.144995\n",
       "9997  19997   6.858170  3.687651   8.885298  3.943446\n",
       "9998  19998  12.892524  4.579880   8.060315  3.944395\n",
       "9999  19999   4.042493  5.651729   9.960974  2.283841\n",
       "\n",
       "[10000 rows x 5 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.to_csv('./submission/sub_0.78230.csv', index=False)\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(permute_models, cv):\n",
    "    for label in permute_models:\n",
    "        for n, i in enumerate(permute_models[label]):\n",
    "            joblib.dump(i, './models/%s_%sfold_%s.pkl'%(label,n,cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(permute_models, '0.78230')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prob값을 feature로 사용하여 모델 재학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_model(x_train, y_train,label):\n",
    "    models=[]\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    oof = np.zeros((x_train.shape[0],)) # oof\n",
    "\n",
    "    # train, test split\n",
    "    for n_fold, (trn_idx, val_idx) in enumerate(kf.split(x_train)):\n",
    "\n",
    "        #print(n_fold)\n",
    "        trn_x, trn_y = x_train.iloc[trn_idx], y_train.iloc[trn_idx]\n",
    "        val_x, val_y = x_train.iloc[val_idx], y_train.iloc[val_idx]\n",
    "\n",
    "\n",
    "        model= joblib.load('./models/%s_%sfold_0.78230.pkl'%(label,n_fold))\n",
    "        \n",
    "\n",
    "        # OOF\n",
    "        v_p = model.predict(val_x)\n",
    "        oof[val_idx] = v_p\n",
    "\n",
    "    \n",
    "        \n",
    "    return oof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels = ['hhb', 'hbo2', 'ca', 'na']\n",
    "threshold = [0.001, 0.0005, 0.0001]\n",
    "oofs=[]\n",
    "for label in labels:\n",
    "    permute_features = pd.read_csv('./bad_features/%s_bad_features.csv'%label,  names=['feature','count'])   \n",
    "    bad_features = permute_features[permute_features['count'] >1]['feature'].values     \n",
    "    \n",
    "    x_reduced_trn = x_train.drop(bad_features, axis=1).copy()\n",
    "    x_reduced_test = test.drop(bad_features, axis=1).copy()\n",
    "    y_train2 = y_train[label].copy()\n",
    "    \n",
    "    oof=predict_model(x_reduced_trn, y_train2, label)\n",
    "    oofs.append(oof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7822988851223733"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[]\n",
    "for label,i in zip(labels,oofs):\n",
    "    a.append(mean_absolute_error(train[label], i))\n",
    "np.mean(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_features(x_train, oofs, data='train'):\n",
    "    \"\"\"\n",
    "    proba features 추가\n",
    "    \"\"\"\n",
    "    if data =='train':\n",
    "        preds_hhb = oofs[0]\n",
    "        preds_hbo2 = oofs[1]\n",
    "        preds_ca = oofs[2]\n",
    "        preds_na = oofs[3]\n",
    "    elif data =='test':\n",
    "        preds_hhb = oofs['hhb']\n",
    "        preds_hbo2 = oofs['hbo2']\n",
    "        preds_ca = oofs['ca']\n",
    "        preds_na = oofs['na']        \n",
    "\n",
    "    x_train['hhb_prob'] = preds_hhb\n",
    "    x_train['hbo2_prob'] = preds_hbo2\n",
    "    x_train['ca_prob'] = preds_ca\n",
    "    x_train['na_prob'] = preds_na\n",
    "\n",
    "    x_train['hhb/hbo2'] = preds_hhb/preds_hbo2\n",
    "    x_train['hhb/ca'] = preds_hhb/preds_ca\n",
    "    x_train['hhb/na'] = preds_hhb/preds_na\n",
    "    x_train['hbo2/ca'] = preds_hbo2/preds_ca\n",
    "    x_train['hbo2/na'] = preds_hbo2/preds_na\n",
    "    x_train['ca/na'] = preds_ca/preds_na\n",
    "    \n",
    "    return x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "x_train = prob_features(x_train, oofs, data='train')\n",
    "\n",
    "# test\n",
    "stack_sub = pd.read_csv('./submission/sub_0.78230.csv')\n",
    "test = prob_features(test, stack_sub, data='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain_model(x_train, y_train, x_test, seed, label, cv):\n",
    "    models=[]\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    oof = np.zeros((x_train.shape[0],)) # oof\n",
    "    pred = np.zeros(x_test.shape[0])\n",
    "    # train, test split\n",
    "    for n_fold, (trn_idx, val_idx) in enumerate(kf.split(x_train)):\n",
    "\n",
    "        #print(n_fold)\n",
    "        trn_x, trn_y = x_train.iloc[trn_idx], y_train.iloc[trn_idx]\n",
    "        val_x, val_y = x_train.iloc[val_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        if os.path.isfile('./models/%s_%sfold_%s_seed%s.pkl'%(label, n_fold, cv, seed)): # 모델 존재할경우\n",
    "            print('%s model load..'%label)\n",
    "            model= joblib.load('./models/%s_%sfold_%s_seed%s.pkl'%(label,n_fold, cv, seed))\n",
    "        \n",
    "        else: # 모델 없을때는 train\n",
    "            print('train %s '%label)\n",
    "            model= lgb.LGBMRegressor(boosting_type ='dart', n_estimators=20000,num_leaves=64, max_depth=-1, \n",
    "                                     min_child_weight=5, subsample=0.7, colsample_bytree = 0.2, learning_rate=0.01, gamma = 0 , n_jobs=-1,\n",
    "                                random_state=42,reg_alpha=0.1, reg_lambda=0.1)\n",
    "\n",
    "            model.fit(trn_x, trn_y, eval_set=[(trn_x, trn_y),(val_x, val_y)], \n",
    "                      early_stopping_rounds=50 ,verbose=20000, eval_metric='mae')\n",
    "        \n",
    "            models.append(model)\n",
    "\n",
    "        # OOF\n",
    "        v_p = model.predict(val_x)\n",
    "        oof[val_idx] = v_p\n",
    "        \n",
    "        # PREDS\n",
    "        pred += model.predict(x_test)/5.0\n",
    "    \n",
    "        \n",
    "    return models, oof, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(seed, cv):\n",
    "    models_re = {}\n",
    "    oofs_re=[]\n",
    "    preds_re=[]\n",
    "    threshold = [0.001, 0.0005, 0.0001]\n",
    "\n",
    "    for label in ['hhb', 'hbo2', 'ca', 'na']:\n",
    "        print('train column : ', label)\n",
    "        pred = np.zeros(test.shape[0])\n",
    "        permute_features = pd.read_csv('./bad_features/%s_bad_features.csv'%label,  names=['feature','count'])   \n",
    "        bad_features = permute_features[permute_features['count'] >1]['feature'].values     \n",
    "    \n",
    "        x_reduced_trn = x_train.drop(bad_features, axis=1).copy()\n",
    "        x_reduced_test = test.drop(bad_features, axis=1).copy()\n",
    "\n",
    "        print(x_reduced_trn.shape[1])\n",
    "        ms, oof, pred =retrain_model(x_reduced_trn, y_train[label], x_reduced_test, seed, label, cv)\n",
    "\n",
    "\n",
    "        models_re[label] = ms\n",
    "        oofs_re.append(oof)\n",
    "        preds_re.append(pred)\n",
    "    \n",
    "    return models_re, oofs_re, preds_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train column :  hhb\n",
      "290\n",
      "hhb model load..\n",
      "hhb model load..\n",
      "hhb model load..\n",
      "hhb model load..\n",
      "hhb model load..\n",
      "train column :  hbo2\n",
      "246\n",
      "hbo2 model load..\n",
      "hbo2 model load..\n",
      "hbo2 model load..\n",
      "hbo2 model load..\n",
      "hbo2 model load..\n",
      "train column :  ca\n",
      "288\n",
      "ca model load..\n",
      "ca model load..\n",
      "ca model load..\n",
      "ca model load..\n",
      "ca model load..\n",
      "train column :  na\n",
      "172\n",
      "na model load..\n",
      "na model load..\n",
      "na model load..\n",
      "na model load..\n",
      "na model load..\n",
      "train column :  hhb\n",
      "290\n",
      "hhb model load..\n",
      "hhb model load..\n",
      "hhb model load..\n",
      "hhb model load..\n",
      "hhb model load..\n",
      "train column :  hbo2\n",
      "246\n",
      "hbo2 model load..\n",
      "hbo2 model load..\n",
      "hbo2 model load..\n",
      "hbo2 model load..\n",
      "hbo2 model load..\n",
      "train column :  ca\n",
      "288\n",
      "ca model load..\n",
      "ca model load..\n",
      "ca model load..\n",
      "ca model load..\n",
      "ca model load..\n",
      "train column :  na\n",
      "172\n",
      "na model load..\n",
      "na model load..\n",
      "na model load..\n",
      "na model load..\n",
      "na model load..\n",
      "train column :  hhb\n",
      "290\n",
      "hhb model load..\n",
      "hhb model load..\n",
      "hhb model load..\n",
      "hhb model load..\n",
      "hhb model load..\n",
      "train column :  hbo2\n",
      "246\n",
      "hbo2 model load..\n",
      "hbo2 model load..\n",
      "hbo2 model load..\n",
      "hbo2 model load..\n",
      "hbo2 model load..\n",
      "train column :  ca\n",
      "288\n",
      "ca model load..\n",
      "ca model load..\n",
      "ca model load..\n",
      "ca model load..\n",
      "ca model load..\n",
      "train column :  na\n",
      "172\n",
      "na model load..\n",
      "na model load..\n",
      "na model load..\n",
      "na model load..\n",
      "na model load..\n"
     ]
    }
   ],
   "source": [
    "seed=[42,92,2020] ; cv = [0.79025, 0.77943, 0.78182]\n",
    "models_re1, oofs_re1, preds_re1 = train_model(seed[0], cv[0])\n",
    "models_re2, oofs_re2, preds_re2 = train_model(seed[1], cv[1])\n",
    "models_re3, oofs_re3, preds_re3 = train_model(seed[2], cv[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49621931148554294, 0.4027651102521572, 1.251376674299484, 1.01063331532915]\n",
      "42 seed oof mae 0.79025\n"
     ]
    }
   ],
   "source": [
    "labels = ['hhb', 'hbo2', 'ca', 'na']\n",
    "a=[]\n",
    "for label,i in zip(labels,oofs_re1):\n",
    "    a.append(mean_absolute_error(train[label], i))\n",
    "print(a)\n",
    "print('42 seed oof mae %.5f'% np.mean(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.48738207291747193, 0.3963998804157671, 1.2345040134839655, 0.9994471082980501]\n",
      "92 seed oof mae 0.77943\n"
     ]
    }
   ],
   "source": [
    "a=[]\n",
    "for label,i in zip(labels,oofs_re2):\n",
    "    a.append(mean_absolute_error(train[label], i))\n",
    "print(a)\n",
    "print('92 seed oof mae %.5f'% np.mean(a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.48610003505209726, 0.39494450071581094, 1.2415459878944304, 1.0046876782622796]\n",
      "2020 seed oof mae 0.78182\n"
     ]
    }
   ],
   "source": [
    "a=[]\n",
    "for label,i in zip(labels,oofs_re3):\n",
    "    a.append(mean_absolute_error(train[label], i))\n",
    "print(a)\n",
    "print('2020 seed oof mae %.5f'% np.mean(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4848126801392114, 0.3951887481523925, 1.2348176409768281, 1.000027085577363]\n",
      "oof mae 0.77871\n"
     ]
    }
   ],
   "source": [
    "# 최종 oof 성능\n",
    "final_oof=[]\n",
    "for i in range(4):\n",
    "    temp_oof=np.zeros(train.shape[0])\n",
    "    temp_oof+= (oofs_re1[i]+oofs_re2[i]+oofs_re3[i])/3.0\n",
    "    final_oof.append(temp_oof)\n",
    "#  \n",
    "a=[]\n",
    "for label,i in zip(labels,final_oof):\n",
    "    a.append(mean_absolute_error(train[label], i))\n",
    "print(a)\n",
    "print('oof mae %.5f'% np.mean(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>hhb</th>\n",
       "      <th>hbo2</th>\n",
       "      <th>ca</th>\n",
       "      <th>na</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000</td>\n",
       "      <td>8.139906</td>\n",
       "      <td>5.050496</td>\n",
       "      <td>9.591441</td>\n",
       "      <td>2.897216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>7.832381</td>\n",
       "      <td>2.404754</td>\n",
       "      <td>8.636410</td>\n",
       "      <td>2.364584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10002</td>\n",
       "      <td>9.797005</td>\n",
       "      <td>5.144637</td>\n",
       "      <td>9.655771</td>\n",
       "      <td>3.127631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10003</td>\n",
       "      <td>8.033548</td>\n",
       "      <td>4.509453</td>\n",
       "      <td>8.850021</td>\n",
       "      <td>4.441217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10004</td>\n",
       "      <td>7.119349</td>\n",
       "      <td>2.595708</td>\n",
       "      <td>9.248665</td>\n",
       "      <td>3.407534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>19995</td>\n",
       "      <td>3.628231</td>\n",
       "      <td>4.391384</td>\n",
       "      <td>12.157612</td>\n",
       "      <td>5.364728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>19996</td>\n",
       "      <td>10.684080</td>\n",
       "      <td>2.923970</td>\n",
       "      <td>8.306135</td>\n",
       "      <td>3.141520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>19997</td>\n",
       "      <td>6.668286</td>\n",
       "      <td>3.735521</td>\n",
       "      <td>9.247005</td>\n",
       "      <td>4.048194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>19998</td>\n",
       "      <td>12.813112</td>\n",
       "      <td>4.595704</td>\n",
       "      <td>7.960818</td>\n",
       "      <td>4.061691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>19999</td>\n",
       "      <td>4.024079</td>\n",
       "      <td>5.825145</td>\n",
       "      <td>9.511309</td>\n",
       "      <td>2.095771</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id        hhb      hbo2         ca        na\n",
       "0     10000   8.139906  5.050496   9.591441  2.897216\n",
       "1     10001   7.832381  2.404754   8.636410  2.364584\n",
       "2     10002   9.797005  5.144637   9.655771  3.127631\n",
       "3     10003   8.033548  4.509453   8.850021  4.441217\n",
       "4     10004   7.119349  2.595708   9.248665  3.407534\n",
       "...     ...        ...       ...        ...       ...\n",
       "9995  19995   3.628231  4.391384  12.157612  5.364728\n",
       "9996  19996  10.684080  2.923970   8.306135  3.141520\n",
       "9997  19997   6.668286  3.735521   9.247005  4.048194\n",
       "9998  19998  12.813112  4.595704   7.960818  4.061691\n",
       "9999  19999   4.024079  5.825145   9.511309  2.095771\n",
       "\n",
       "[10000 rows x 5 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# submission\n",
    "submission_gwang = pd.read_csv('./sample_submission.csv')\n",
    "submission_gwang['hhb'] = (preds_re1[0] +preds_re2[0] +preds_re3[0]) /3\n",
    "submission_gwang['hbo2'] = (preds_re1[1] +preds_re2[1] +preds_re3[1]) /3\n",
    "submission_gwang['ca'] = (preds_re1[2] +preds_re2[2] +preds_re3[2]) /3\n",
    "submission_gwang['na'] = (preds_re1[3] +preds_re2[3] +preds_re3[3]) /3\n",
    "\n",
    "submission_gwang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_gwang.to_csv('./submission/0.77871_gwang_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"모델 저장\"\"\"\n",
    "\n",
    "def save_model(permute_models, cv, seed=42):\n",
    "    for label in permute_models:\n",
    "        for n, i in enumerate(permute_models[label]):\n",
    "            joblib.dump(i, './models/%s_%sfold_%s_seed%s.pkl'%(label,n, cv, seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(models_re1, '0.79025', seed=42)\n",
    "save_model(models_re2, '0.77943', seed=92)\n",
    "save_model(models_re3, '0.78182', seed=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 기찬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 라이브러리 및 데이터\n",
    "## Library & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle, os\n",
    "import joblib\n",
    "from tqdm import trange, tqdm, tqdm_notebook\n",
    "\n",
    "# DATA SPLIT\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "# EVALUATE\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# MODEL\n",
    "import lightgbm as lgbm\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import xgboost as xgb\n",
    "# ELSE\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./train.csv\", index_col = 0)\n",
    "test = pd.read_csv(\"./test.csv\", index_col=0)\n",
    "\n",
    "df = train.append(test)\n",
    "df = df[train.columns]\n",
    "submission = pd.read_csv('./sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 변수 선택 및 모델 구축\n",
    "## Feature Engineering & Initial Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_col=train.columns[train.columns.str.contains('src')]\n",
    "dst_col = train.columns[train.columns.str.contains('dst')]\n",
    "\n",
    "def fe3(train):\n",
    "    train['src_mean']=train[src_col].mean(1)\n",
    "    train['dst_mean']=train[dst_col].mean(1)\n",
    "    train['div_feature_src2'] = train['dst_mean']/train['src_mean']\n",
    "\n",
    "    train['src_std']=train[src_col].std(1)\n",
    "    train['dst_std']=train[dst_col].std(1)\n",
    "    train['div_feature_dst2'] = train['dst_std']/train['src_std']\n",
    "\n",
    "    \n",
    "    return train\n",
    "\n",
    "train = fe3(train)\n",
    "test = fe3(test)\n",
    "\n",
    "############################################################\n",
    "\n",
    "def fe_temp(train, src_col, dst_col, size=50):\n",
    "    for i in range(650, 1000-size, size):\n",
    "        temp1=train.loc[:, '%s_src'%i : '%s_src'%(i+size)].mean(1)\n",
    "        temp2=train.loc[:, '%s_dst'%i : '%s_dst'%(i+size)].mean(1)\n",
    "        train['%s_to_%s_mean_src_dst_div'%(i,i+size)] = temp2/temp1\n",
    "        train['%s_to_%s_mean_src_dst_div_rho'%(i,i+size)] = np.log(temp2/temp1) / train['rho']\n",
    "    \n",
    "    return train\n",
    "\n",
    "for i in [10,30,50,100]:\n",
    "    train = fe_temp(train, src_col, dst_col, size=i)\n",
    "    test = fe_temp(test, src_col, dst_col, size=i)\n",
    "############################################################\n",
    "temp = [x for x in range(650,1000,10)]\n",
    "\n",
    "def window_rollinng_first(train):\n",
    "    for k in [3, 5]:#window size\n",
    "        temp1 = train[src_col].rolling(window = k, min_periods=1, axis=1).mean()\n",
    "        temp2 = train[dst_col].rolling(window = k, min_periods=1, axis=1).mean()\n",
    "        for i in temp:\n",
    "            train['%s_rolling_win_%s'%(i, k)] = np.log(temp2['%s_dst'%i]/temp1['%s_src'%i])/train['rho']\n",
    "        \n",
    "    return train\n",
    "\n",
    "train = window_rollinng_first(train)\n",
    "test = window_rollinng_first(test)\n",
    "\n",
    "############################################################\n",
    "temp = [x for x in range(650,1000,10)]\n",
    "\n",
    "def window_rolling_div(train):\n",
    "    for i in temp:\n",
    "        train['%s_rolling_win3/win5'%i] = train['%s_rolling_win_3'%i]/ train['%s_rolling_win_5'%i]\n",
    "        \n",
    "    return train\n",
    "\n",
    "train = window_rolling_div(train)\n",
    "test = window_rolling_div(test)\n",
    "\n",
    "############################################################\n",
    "def window_rolling_div_near(train, near):\n",
    "    temp = [x for x in range(650,1000-near, 10)]\n",
    "    for i in temp:\n",
    "        train['near_%s_%s_win_3'%(i,i+near)] = train['%s_rolling_win_%s'%(i, 3)] / train['%s_rolling_win_%s'%(i+near, 3)]\n",
    "        train['near_%s_%s_win_5'%(i,i+near)] = train['%s_rolling_win_%s'%(i, 5)] / train['%s_rolling_win_%s'%(i+near, 5)]\n",
    "    return train\n",
    "\n",
    "for i in range(10, 30, 10):\n",
    "    train = window_rolling_div_near(train,i)\n",
    "    test = window_rolling_div_near(test, i)\n",
    "############################################################\n",
    "temp = [x for x in range(650,1000,10)]\n",
    "\n",
    "def temp_fe(train):\n",
    "    for i in temp:\n",
    "        train['%s_dd'%i] = train['%s_dst'%i]/train['%s_src'%i]\n",
    "        train['%s_dd2'%i] = np.log(train['%s_dst'%i]/train['%s_src'%i])/train['rho']\n",
    "    return train\n",
    "\n",
    "train = temp_fe(train)\n",
    "test = temp_fe(test)\n",
    "\n",
    "############################################################\n",
    "def near_all(train):\n",
    "    temp = [x for x in range(650,1000,10)]\n",
    "    temp_col = []\n",
    "    for i in temp:\n",
    "        temp_col.append('%s_dd2'%i)\n",
    "\n",
    "    for i in temp_col:\n",
    "        for j in temp_col:\n",
    "            if i!=j:\n",
    "                train['%s_%s_near_all_dd2'%(i,j)] = train[i]/train[j]\n",
    "    \n",
    "    return train\n",
    "\n",
    "train = near_all(train)\n",
    "test = near_all(test)\n",
    "\n",
    "############################################################\n",
    "def near_all(train):\n",
    "    temp = [x for x in range(650,1000,10)]\n",
    "    temp_col = []\n",
    "    for i in temp:\n",
    "        temp_col.append('%s_dd2'%i)\n",
    "\n",
    "    for i in temp_col:\n",
    "        for j in temp_col:\n",
    "            if i!=j:\n",
    "                train['%s_%s_near_all_dd3'%(i,j)] = train[i]-train[j]\n",
    "    \n",
    "    return train\n",
    "\n",
    "#train = near_all(train)\n",
    "#test = near_all(test)\n",
    "############################################################\n",
    "\n",
    "def exp1(train, size=10):\n",
    "    temp_col=[]\n",
    "    for i in [x for x in range(650, 1000-size, size)]:\n",
    "        temp_col.append('%s_to_%s_mean_src_dst_div_rho'%(i,i+size))\n",
    "        \n",
    "    for i in temp_col:\n",
    "        for j in temp_col:\n",
    "            if i!=j:\n",
    "                train['fucking_%s_%s'%(i,j)] = train[i]/train[j]\n",
    "                #train['fucking2_%s_%s'%(i,j)] = train[i]-train[j]\n",
    "    return train\n",
    "\n",
    "train = exp1(train, size=10)\n",
    "test = exp1(test, size=10)\n",
    "\n",
    "train = exp1(train, size=30)\n",
    "test = exp1(test, size=30)\n",
    "train = exp1(train, size=50)\n",
    "test = exp1(test, size=50)\n",
    "train = exp1(train, size=100)\n",
    "test = exp1(test, size=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=test.replace([np.inf, -np.inf], np.nan)\n",
    "train=train.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "df1 = train.append(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering & Initial Modeling 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./train.csv\", index_col = 0)\n",
    "test = pd.read_csv(\"./test.csv\", index_col=0)\n",
    "\n",
    "df = train.append(test)\n",
    "df = df[train.columns]\n",
    "submission = pd.read_csv('./sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "kind = ['dst','src']\n",
    "X_dst = df.iloc[:,np.where(df.columns.str.find(kind[0]) == 4)[0]].copy()\n",
    "X_src = df.iloc[:,np.where(df.columns.str.find(kind[1]) == 4)[0]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_absor_rho(df, X_dst, X_src, window):\n",
    "\n",
    "    temp1 = X_dst.copy()\n",
    "    temp2 = X_src.copy()\n",
    "\n",
    "    temp1.columns = [i for i in range(0, len(temp1.columns))]\n",
    "    temp2.columns = [i for i in range(0, len(temp2.columns))]\n",
    "    \n",
    "    for i in range(0, len(X_dst)):\n",
    "        temp1.iloc[i,:] = np.log(np.array(temp1.iloc[i,:].rolling(window=window,min_periods=1).mean())/\\\n",
    "                                 np.array(temp2.iloc[i,:].rolling(window=window,min_periods=1).mean()))/df.loc[i,'rho']\n",
    "        \n",
    "    absor_rolling = temp1.copy()\n",
    "    absor_rolling.columns =  X_dst.columns + \"/\" + X_src.columns + \"_rho_\" + str(window)\n",
    "    \n",
    "    \n",
    "    absor_rolling = absor_rolling.replace(np.inf, np.nan)\n",
    "    absor_rolling = absor_rolling.replace(-np.inf, np.nan)\n",
    "    \n",
    "    absor_rolling[\"rolling_mean_\" + str(window)] = absor_rolling.mean(axis=1)\n",
    "    absor_rolling[\"rolling_std_\" + str(window)] = absor_rolling.std(axis=1)   \n",
    "    \n",
    "    \n",
    "    return absor_rolling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_absor(df, X_dst, X_src, window):\n",
    "\n",
    "    temp1 = X_dst.copy()\n",
    "    temp2 = X_src.copy()\n",
    "\n",
    "    temp1.columns = [i for i in range(0, len(temp1.columns))]\n",
    "    temp2.columns = [i for i in range(0, len(temp2.columns))]\n",
    "    \n",
    "    for i in range(0, len(X_dst)):\n",
    "        temp1.iloc[i,:] = np.array(temp1.iloc[i,:].rolling(window=window,min_periods=1).mean())/\\\n",
    "                                 np.array(temp2.iloc[i,:].rolling(window=window,min_periods=1).mean())\n",
    "        \n",
    "    absor_rolling = temp1.copy()\n",
    "    absor_rolling.columns =  X_dst.columns + \"/\" + X_src.columns + \"_\" + str(window)\n",
    "    \n",
    "    \n",
    "    absor_rolling = absor_rolling.replace(np.inf, np.nan)\n",
    "    absor_rolling = absor_rolling.replace(-np.inf, np.nan)\n",
    "    \n",
    "    absor_rolling[\"rolling_mean_\" + str(window)] = absor_rolling.mean(axis=1)\n",
    "    absor_rolling[\"rolling_std_\" + str(window)] = absor_rolling.std(axis=1)   \n",
    "    \n",
    "    \n",
    "    return absor_rolling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_each_col(df, stride):\n",
    "    temp = df.copy()\n",
    "    for i in range(stride, len(temp.columns)):\n",
    "        temp[temp.columns[i-stride] + \"/\" + temp.columns[i]] = temp[temp.columns[i-stride]]/temp[temp.columns[i]]\n",
    "        \n",
    "    return temp.iloc[:,len(df.columns):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtract_each_col(df, stride):\n",
    "    temp = df.copy()\n",
    "    for i in range(stride, len(temp.columns)):\n",
    "        temp[temp.columns[i-stride] + \"-\" + temp.columns[i]] = temp[temp.columns[i-stride]] - temp[temp.columns[i]]\n",
    "        \n",
    "    return temp.iloc[:,len(df.columns):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_df(df1, df2):\n",
    "    temp1 = df1.copy()\n",
    "    temp2 = df2.copy()\n",
    "    \n",
    "    temp1.columns = [i for i in range(0, len(temp1.columns))]\n",
    "    temp2.columns = [i for i in range(0, len(temp2.columns))]\n",
    "    \n",
    "    df = temp1/temp2\n",
    "    df.columns = df1.columns + \"/\" + df2.columns\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "absor_rolling1 = get_absor(df, X_dst, X_src, 1)\n",
    "absor_rolling2 = get_absor(df, X_dst, X_src, 3)\n",
    "absor_rolling3 = get_absor(df, X_dst, X_src, 5)\n",
    "absor_rolling4 = get_absor(df, X_dst, X_src, 10)\n",
    "\n",
    "absor_rho_rolling1 = get_absor_rho(df, X_dst, X_src, 1)\n",
    "absor_rho_rolling2 = get_absor_rho(df, X_dst, X_src, 3)\n",
    "absor_rho_rolling3 = get_absor_rho(df, X_dst, X_src, 5)\n",
    "absor_rho_rolling4 = get_absor_rho(df, X_dst, X_src, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "absor_rolling1_divide_stride1 = divide_each_col(absor_rolling1, 1)\n",
    "absor_rolling1_divide_stride2 = divide_each_col(absor_rolling1, 3)\n",
    "absor_rolling1_divide_stride3 = divide_each_col(absor_rolling1, 5)\n",
    "absor_rolling1_divide_stride4 = divide_each_col(absor_rolling1, 10)\n",
    "\n",
    "absor_rolling2_divide_stride1 = divide_each_col(absor_rolling2, 1)\n",
    "absor_rolling2_divide_stride2 = divide_each_col(absor_rolling2, 3)\n",
    "absor_rolling2_divide_stride3 = divide_each_col(absor_rolling2, 5)\n",
    "absor_rolling2_divide_stride4 = divide_each_col(absor_rolling2, 10)\n",
    "\n",
    "absor_rolling3_divide_stride1 = divide_each_col(absor_rolling3, 1)\n",
    "absor_rolling3_divide_stride2 = divide_each_col(absor_rolling3, 3)\n",
    "absor_rolling3_divide_stride3 = divide_each_col(absor_rolling3, 5)\n",
    "absor_rolling3_divide_stride4 = divide_each_col(absor_rolling3, 10)\n",
    "\n",
    "absor_rolling4_divide_stride1 = divide_each_col(absor_rolling4, 1)\n",
    "absor_rolling4_divide_stride2 = divide_each_col(absor_rolling4, 3)\n",
    "absor_rolling4_divide_stride3 = divide_each_col(absor_rolling4, 5)\n",
    "absor_rolling4_divide_stride4 = divide_each_col(absor_rolling4, 10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########3\n",
    "\n",
    "\n",
    "\n",
    "absor_rho_rolling1_divide_stride1 = divide_each_col(absor_rho_rolling1, 1)\n",
    "absor_rho_rolling1_divide_stride2 = divide_each_col(absor_rho_rolling1, 3)\n",
    "absor_rho_rolling1_divide_stride3 = divide_each_col(absor_rho_rolling1, 5)\n",
    "absor_rho_rolling1_divide_stride4 = divide_each_col(absor_rho_rolling1, 10)\n",
    "\n",
    "absor_rho_rolling2_divide_stride1 = divide_each_col(absor_rho_rolling2, 1)\n",
    "absor_rho_rolling2_divide_stride2 = divide_each_col(absor_rho_rolling2, 3)\n",
    "absor_rho_rolling2_divide_stride3 = divide_each_col(absor_rho_rolling2, 5)\n",
    "absor_rho_rolling2_divide_stride4 = divide_each_col(absor_rho_rolling2, 10)\n",
    "\n",
    "absor_rho_rolling3_divide_stride1 = divide_each_col(absor_rho_rolling3, 1)\n",
    "absor_rho_rolling3_divide_stride2 = divide_each_col(absor_rho_rolling3, 3)\n",
    "absor_rho_rolling3_divide_stride3 = divide_each_col(absor_rho_rolling3, 5)\n",
    "absor_rho_rolling3_divide_stride4 = divide_each_col(absor_rho_rolling3, 10)\n",
    "\n",
    "absor_rho_rolling4_divide_stride1 = divide_each_col(absor_rho_rolling4, 1)\n",
    "absor_rho_rolling4_divide_stride2 = divide_each_col(absor_rho_rolling4, 3)\n",
    "absor_rho_rolling4_divide_stride3 = divide_each_col(absor_rho_rolling4, 5)\n",
    "absor_rho_rolling4_divide_stride4 = divide_each_col(absor_rho_rolling4, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "absor_rho_rolling_1_divide_rolling_2 = divide_df(absor_rho_rolling1, absor_rho_rolling2)\n",
    "absor_rho_rolling_2_divide_rolling_3 = divide_df(absor_rho_rolling2, absor_rho_rolling3)\n",
    "absor_rho_rolling_3_divide_rolling_4 = divide_df(absor_rho_rolling3, absor_rho_rolling4)\n",
    "\n",
    "absor_rolling_1_divide_rolling_2 = divide_df(absor_rolling1, absor_rolling2)\n",
    "absor_rolling_2_divide_rolling_3 = divide_df(absor_rolling2, absor_rolling3)\n",
    "absor_rolling_3_divide_rolling_4 = divide_df(absor_rolling3, absor_rolling4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, absor_rolling1, on='id')\n",
    "df = pd.merge(df, absor_rolling2, on='id')\n",
    "df = pd.merge(df, absor_rolling3, on='id')\n",
    "df = pd.merge(df, absor_rolling4, on='id')\n",
    "\n",
    "df = pd.merge(df, absor_rho_rolling1, on='id')\n",
    "df = pd.merge(df, absor_rho_rolling2, on='id')\n",
    "df = pd.merge(df, absor_rho_rolling3, on='id')\n",
    "df = pd.merge(df, absor_rho_rolling4, on='id')\n",
    "\n",
    "##\n",
    "df = pd.merge(df, absor_rho_rolling1_divide_stride1, on='id')\n",
    "df = pd.merge(df, absor_rho_rolling1_divide_stride2, on='id')\n",
    "df = pd.merge(df, absor_rho_rolling1_divide_stride3, on='id')\n",
    "df = pd.merge(df, absor_rho_rolling1_divide_stride4, on='id')\n",
    "\n",
    "df = pd.merge(df, absor_rho_rolling2_divide_stride1, on='id')\n",
    "df = pd.merge(df, absor_rho_rolling2_divide_stride2, on='id')\n",
    "df = pd.merge(df, absor_rho_rolling2_divide_stride3, on='id')\n",
    "df = pd.merge(df, absor_rho_rolling2_divide_stride4, on='id')\n",
    "\n",
    "df = pd.merge(df, absor_rho_rolling3_divide_stride1, on='id')\n",
    "df = pd.merge(df, absor_rho_rolling3_divide_stride2, on='id')\n",
    "df = pd.merge(df, absor_rho_rolling3_divide_stride3, on='id')\n",
    "df = pd.merge(df, absor_rho_rolling3_divide_stride4, on='id')\n",
    "\n",
    "df = pd.merge(df, absor_rho_rolling4_divide_stride1, on='id')\n",
    "df = pd.merge(df, absor_rho_rolling4_divide_stride2, on='id')\n",
    "df = pd.merge(df, absor_rho_rolling4_divide_stride3, on='id')\n",
    "df = pd.merge(df, absor_rho_rolling4_divide_stride4, on='id')\n",
    "\n",
    "###\n",
    "\n",
    "\n",
    "####\n",
    "\n",
    "df = pd.merge(df, absor_rho_rolling_1_divide_rolling_2, on='id')\n",
    "df = pd.merge(df, absor_rho_rolling_2_divide_rolling_3, on='id')\n",
    "df = pd.merge(df, absor_rho_rolling_3_divide_rolling_4, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace(np.inf, np.nan)\n",
    "df = df.replace(-np.inf, np.nan)\n",
    "\n",
    "df2 = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 모델 학습 및 검증\n",
    "## Model Tuning & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(df):\n",
    "\n",
    "    X_train = df.drop(['hhb','hbo2','ca','na'], axis=1).loc[0:9999].copy()\n",
    "    X_test = df.drop(['hhb','hbo2','ca','na'], axis=1).loc[10000:20000].copy()\n",
    "\n",
    "    y_train = df[['hhb','hbo2','ca','na']].loc[0:9999].copy()\n",
    "    y_test = df[['hhb','hbo2','ca','na']].loc[10000:20000].copy()\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train1, X_test1, y_train1, y_test1 = get_train_test(df1)\n",
    "X_train2, X_test2, y_train2, y_test2 = get_train_test(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from eli5.permutation_importance import get_score_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'objective':'regression','n_estimators':10000, 'learning_rate':0.01,'random_state':42,\n",
    "        'early_stopping_rounds':50,'colsample_bytree':0.5, 'metric':'l1'} #'\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=777)\n",
    "\n",
    "def my_permutation_badfeatures(X_train, y_train, col, kf):\n",
    "    y_train_temp = y_train[col].copy()\n",
    "    threshold = [0.001, 0.0001]\n",
    "    bad_features1 = []\n",
    "    bad_features2 = []\n",
    "    #bad_features3 = []\n",
    "        \n",
    "    def score(X, y):\n",
    "        y_pred = reg.predict(X)\n",
    "        return abs(y-y_pred).mean() \n",
    "    \n",
    "    for n_fold, (trn_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "        trn_x, trn_y = X_train.iloc[trn_idx], y_train_temp.iloc[trn_idx]\n",
    "        val_x, val_y = X_train.iloc[val_idx], y_train_temp.iloc[val_idx]\n",
    "\n",
    "        \n",
    "        reg = lgbm.LGBMRegressor(**param)\n",
    "        reg.fit(trn_x, trn_y, eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
    "                      verbose=True, eval_metric='mae', early_stopping_rounds=50)\n",
    "        \n",
    "        \n",
    "        base_score, score_decreases = get_score_importances(score,np.array(val_x), np.array(val_y), n_iter=2)\n",
    "        \n",
    "        bad_features1.extend(list(val_x.columns[score_decreases[0] > -threshold[0]]))\n",
    "                             \n",
    "    return bad_features1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=777)\n",
    "\n",
    "##1\n",
    "for col in y_train1.columns:\n",
    "    \n",
    "    \n",
    "    bad_features1 = my_permutation_badfeatures(X_train1, y_train1, col, kf)\n",
    "    pd.DataFrame(bad_features1)[0].value_counts().to_csv(\"./bad_features/bad_features_gwang_\" + col + \"_1.csv\")\n",
    "\n",
    "##2\n",
    "for col in y_train2.columns:\n",
    "    \n",
    "    \n",
    "    bad_features1 = my_permutation_badfeatures(X_train2, y_train2, col, kf)\n",
    "    pd.DataFrame(bad_features1)[0].value_counts().to_csv(\"./bad_features/bad_features_gichan_\" + col + \"_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'objective':'reg:squarederror','eval_metric':'mae','colsample_bytree':0.7,'learning_rate':0.01,\n",
    "          'n_estimators':20000,'random_state':42, 'tree_method':'gpu_hist','n_gpus':1,'early_stopping_rounds':50}\n",
    "\n",
    "def my_permutation_lgb_for_stacking(X_train, X_test, y_train, y_test, param, kf, col):\n",
    "\n",
    "    score = []\n",
    "    \n",
    "    y_val_pred = y_train.copy()\n",
    "    y_val_pred.loc[:,:] = 0\n",
    "    \n",
    "    y_submit = y_test.copy()\n",
    "    y_submit.loc[:,:] = 0\n",
    "    \n",
    "    \n",
    "    valid_mae = []\n",
    "    \n",
    "    y_train_temp = y_train[col]\n",
    "    \n",
    "    \n",
    "\n",
    "    for n_fold, (trn_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "        trn_x, trn_y = X_train.iloc[trn_idx], y_train_temp.iloc[trn_idx]\n",
    "        val_x, val_y = X_train.iloc[val_idx], y_train_temp.iloc[val_idx]\n",
    "        \n",
    "        reg = lgbm.LGBMRegressor(**param)\n",
    "\n",
    "    \n",
    "        reg.fit(trn_x, trn_y,\n",
    "                    eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
    "                    early_stopping_rounds=50, verbose=100)\n",
    "    \n",
    "        pred = reg.predict(X_test)\n",
    "        y_submit[col] += pred/5\n",
    "        y_val_pred.loc[val_idx,col] = reg.predict(val_x)\n",
    "        \n",
    "        pred_val = reg.predict(val_x)\n",
    "        valid_mae.append(abs(pred_val-val_y).mean())\n",
    "        \n",
    "        \n",
    "    return y_submit[col], np.array(valid_mae).mean(), y_val_pred[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.106439119498514\n",
      "0.7655641973038987\n",
      "2.274564558091105\n",
      "1.4641019687347696\n",
      "2.1057222429480076\n",
      "0.7683808929313594\n",
      "2.288236002426401\n",
      "1.4585054585418429\n"
     ]
    }
   ],
   "source": [
    "y_submit_lgb1 = y_test1.copy()\n",
    "y_val_pred1 = y_train1.copy(); y_val_pred1.iloc[:,:] = 0\n",
    "\n",
    "y_submit_lgb2 = y_test2.copy()\n",
    "y_val_pred2 = y_train2.copy(); y_val_pred2.iloc[:,:] = 0\n",
    "\n",
    "## seed ensemble 실시한다 곧. random_stae = 777, 777^2, 777^4 \n",
    "param3 = {'objective':'regression','n_estimators':10000, 'learning_rate':0.15, 'random_state':777,\n",
    "        'early_stopping_rounds':50,'colsample_bytree':0.2, 'metric':'l1','boosting':'dart'} #'boosting':'dart'\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=777)\n",
    "\n",
    "\n",
    "## 1\n",
    "valid_score = []\n",
    "for col in ['hhb','hbo2','ca','na']:\n",
    "     \n",
    "    bad_features = pd.read_csv(\"./bad_features_gwang_\" + col + \"_\"+ str(1) + \".csv\", index_col=0).copy()\n",
    "    bad_features = bad_features[bad_features.iloc[:,0] == 5].index\n",
    "    List = list(bad_features)\n",
    "    \n",
    "    y_submit_lgb1[col], valid_mae, y_val_pred1[col] = my_permutation_lgb_for_stacking(X_train1.drop(List, axis=1), \\\n",
    "                                                      X_test1.drop(List, axis=1),y_train1,  y_test1, param3, kf, col)\n",
    "    valid_score.append(valid_mae)\n",
    "    print(valid_mae)\n",
    "\n",
    "## 3\n",
    "valid_score = []\n",
    "for col in ['hhb','hbo2','ca','na']:\n",
    "     \n",
    "    bad_features = pd.read_csv(\"./bad_features_gichan_\" + col + \"_\"+ str(1) + \".csv\", index_col=0).copy()\n",
    "    bad_features = bad_features[bad_features.iloc[:,0] == 5].index\n",
    "    List = list(bad_features)\n",
    "    \n",
    "    y_submit_lgb2[col], valid_mae, y_val_pred2[col] = my_permutation_lgb_for_stacking(X_train2.drop(List, axis=1), \\\n",
    "                                                      X_test2.drop(List, axis=1),y_train2,  y_test2, param3, kf, col)\n",
    "    valid_score.append(valid_mae)\n",
    "    print(valid_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_stacking1 = y_val_pred1.append(y_submit_lgb1)\n",
    "y_stacking1.columns = y_stacking1.columns + \"_stacking\"\n",
    "\n",
    "y_stacking2 = y_val_pred2.append(y_submit_lgb2)\n",
    "y_stacking2.columns = y_stacking2.columns + \"_stacking\"\n",
    "\n",
    "#저장 \n",
    "y_stacking1.to_csv('./y_stacking1.csv')\n",
    "y_stacking2.to_csv('./y_stacking2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permutation + Stacking + dart + final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_all(y_stacking):\n",
    "    y_stacking_temp = y_stacking.copy()\n",
    "    for i in range(0, len(y_stacking.columns)-1):\n",
    "        for j in range(i+1, len(y_stacking.columns)):\n",
    "            y_stacking_temp[y_stacking.columns[i] + \"/\" + y_stacking.columns[j]] = y_stacking[y_stacking.columns[i]]/y_stacking[y_stacking.columns[j]] \n",
    "    return y_stacking_temp.iloc[:,0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_permutation_lgb(X_train, X_test, y_train, y_test, param, kf, col):\n",
    "\n",
    "    score = []\n",
    "    \n",
    "    y_val_pred = y_train.copy()\n",
    "    y_val_pred.loc[:,:] = 0\n",
    "    \n",
    "    y_submit = y_test.copy()\n",
    "    y_submit.loc[:,:] = 0\n",
    "    \n",
    "    \n",
    "    valid_mae = []\n",
    "    \n",
    "    y_train_temp = y_train[col]\n",
    "    \n",
    "    \n",
    "\n",
    "    for n_fold, (trn_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "        trn_x, trn_y = X_train.iloc[trn_idx], y_train_temp.iloc[trn_idx]\n",
    "        val_x, val_y = X_train.iloc[val_idx], y_train_temp.iloc[val_idx]\n",
    "        \n",
    "        reg = lgbm.LGBMRegressor(**param)\n",
    "\n",
    "    \n",
    "        reg.fit(trn_x, trn_y,\n",
    "                    eval_set=[(trn_x, trn_y), (val_x, val_y)],\n",
    "                    early_stopping_rounds=50, verbose=100)\n",
    "    \n",
    "        pred = reg.predict(X_test)\n",
    "        y_submit[col] += pred/5\n",
    "        y_val_pred.loc[val_idx,col] = reg.predict(val_x)\n",
    "        \n",
    "        pred_val = reg.predict(val_x)\n",
    "        valid_mae.append(abs(pred_val-val_y).mean())\n",
    "        \n",
    "        \n",
    "    return y_submit[col], np.array(valid_mae).mean() , y_val_pred[col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# df1 stacking + df2 stacking -> dart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.drop(['hhb','hbo2','ca','na'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_stacking1= pd.read_csv('./y_stacking1.csv', index_col=0)\n",
    "y_stacking2 = pd.read_csv('./y_stacking2.csv', index_col=0)\n",
    "\n",
    "#y_stacking1.index.name ='id'\n",
    "#y_stacking2.index.name ='id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\ttraining's l1: 0.565774\tvalid_1's l1: 0.669097\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-d71607b67f90>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     y_submit_lgb_final[col], valid_mae, y_val_pred[col] = my_permutation_lgb(X_train,\n\u001b[1;32m---> 43\u001b[1;33m                                                       X_test,y_train,  y_test, param, kf, col)\n\u001b[0m\u001b[0;32m     44\u001b[0m     \u001b[0mvalid_score\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_mae\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_mae\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-39-e18b02698ed8>\u001b[0m in \u001b[0;36mmy_permutation_lgb\u001b[1;34m(X_train, X_test, y_train, y_test, param, kf, col)\u001b[0m\n\u001b[0;32m     25\u001b[0m         reg.fit(trn_x, trn_y,\n\u001b[0;32m     26\u001b[0m                     \u001b[0meval_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrn_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrn_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                     early_stopping_rounds=50, verbose=100)\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\parkgichan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[0;32m    741\u001b[0m                                        \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m                                        \u001b[0mcategorical_feature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 743\u001b[1;33m                                        callbacks=callbacks)\n\u001b[0m\u001b[0;32m    744\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    745\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\parkgichan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[0;32m    598\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m                               \u001b[0mcategorical_feature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m                               callbacks=callbacks)\n\u001b[0m\u001b[0;32m    601\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\parkgichan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    247\u001b[0m                                     evaluation_result_list=None))\n\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m         \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\parkgichan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, train_set, fobj)\u001b[0m\n\u001b[0;32m   1974\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[0;32m   1975\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1976\u001b[1;33m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[0;32m   1977\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mFalse\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1978\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#X_train, X_test, y_train_no, y_test = get_train_test(df_temp_final)\n",
    "y_submit_lgb_final = y_test1.copy()\n",
    "y_val_pred = y_train1.copy(); y_val_pred.iloc[:,:] = 0\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=777)\n",
    "\n",
    "#param = {'objective':'regression','n_estimators':10000, 'learning_rate':0.005, 'random_state':777,\n",
    "#        'early_stopping_rounds':50,'colsample_bytree':0.7, 'metric':'l1','n_jobs':12}\n",
    "param = {'objective':'regression','n_estimators':10000, 'learning_rate':0.15, 'random_state':777**2,\n",
    "        'early_stopping_rounds':50,'colsample_bytree':0.2, 'metric':'l1','n_jobs':12,'boosting':'dart'} \n",
    "\n",
    "# seed : 777, 777*2, 777**2\n",
    "\n",
    "valid_score = []\n",
    "for col in ['hhb','hbo2','ca','na']:\n",
    "    \n",
    "    Y0 = y_stacking1\n",
    "    Y1 = divide_all(y_stacking1)\n",
    "    \n",
    "    Y2 = y_stacking2\n",
    "    Y3 = divide_all(y_stacking2)    \n",
    "    \n",
    "    bad_features1 = pd.read_csv(\"./bad_features_gwang_\" + col + \"_\"+ str(1) + \".csv\", index_col=0).copy()\n",
    "    bad_features1 = bad_features1[bad_features1.iloc[:,0] == 5].index\n",
    "    List1 = list(bad_features1)\n",
    "    \n",
    "    bad_features2 = pd.read_csv(\"./bad_features_gichan_\" + col + \"_\"+ str(1) + \".csv\", index_col=0).copy()\n",
    "    bad_features2 = bad_features2[bad_features2.iloc[:,0] == 5].index\n",
    "    List2 = list(bad_features2)\n",
    "    \n",
    "    df_temp = df1.drop(List1, axis=1)\n",
    "    df_temp2 = df2.drop(List2, axis=1)\n",
    "    \n",
    "    # 합체!!\n",
    "    df_temp_final = pd.merge(df_temp, df_temp2, on='id')\n",
    "    df_temp_final = pd.merge(df_temp_final, Y0, on='id')\n",
    "    df_temp_final = pd.merge(df_temp_final, Y1, on='id')\n",
    "    df_temp_final = pd.merge(df_temp_final, Y2, on='id')\n",
    "    df_temp_final = pd.merge(df_temp_final, Y3, on='id')\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = get_train_test(df_temp_final) \n",
    "    \n",
    "    y_submit_lgb_final[col], valid_mae, y_val_pred[col] = my_permutation_lgb(X_train,\n",
    "                                                      X_test,y_train,  y_test, param, kf, col)\n",
    "    valid_score.append(valid_mae)\n",
    "    print(valid_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_submit_lgb_final.to_csv(\"./submission/submission_gichan.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 광한 기찬 서브미션 앙상블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_submit_lgb_final = pd.read_csv('./submission/submission_gichan.csv')\n",
    "submission_gwang = pd.read_csv('./submission/0.77871_gwang_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = submission_gwang.copy()\n",
    "final[['hhb','hbo2','ca', 'na']] = (submission_gwang[['hhb','hbo2','ca', 'na']] + y_submit_lgb_final[['hhb','hbo2','ca', 'na']])/2\n",
    "final.to_csv('./submission/final_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
